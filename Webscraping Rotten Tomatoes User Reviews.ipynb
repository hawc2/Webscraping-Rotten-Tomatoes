{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# Webscraping Rotten Tomatoes User Reviews (Python 3 + Selenium)\n# Updated 2026 to work with RT's current JavaScript-rendered site.\n#\n# Strategy:\n#   1. Use Selenium to load the user reviews page and click \"Load More\"\n#   2. Intercept internal API (XHR/fetch) responses for clean JSON data\n#   3. Fall back to HTML parsing if API interception doesn't work\n#\n# Requirements:\n#   pip install selenium beautifulsoup4 pandas lxml webdriver-manager\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import (\n    TimeoutException, NoSuchElementException, ElementClickInterceptedException\n)\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport json\nimport time\nimport random\nimport re\nimport os"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Configuration ──────────────────────────────────────────────\n# Change this slug to scrape a different movie.\n# The slug is the part after /m/ in the RT URL, e.g.:\n#   https://www.rottentomatoes.com/m/the_hurt_locker  →  \"the_hurt_locker\"\n\nMOVIE_SLUG = \"the_hurt_locker\"\n\n# Where to save the output CSV (defaults to this notebook's directory)\nOUTPUT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n\n# Set to False to run Chrome visibly (useful for debugging selectors)\nHEADLESS = True"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Helper: create a Selenium Chrome driver ───────────────────\n\ndef make_driver(headless=True):\n    \"\"\"Create and return a Chrome WebDriver with network logging.\"\"\"\n    opts = Options()\n    if headless:\n        opts.add_argument(\"--headless=new\")\n    opts.add_argument(\"--no-sandbox\")\n    opts.add_argument(\"--disable-dev-shm-usage\")\n    opts.add_argument(\"--disable-gpu\")\n    opts.add_argument(\"--window-size=1920,1080\")\n    opts.add_argument(\n        \"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n    )\n    opts.set_capability(\"goog:loggingPrefs\", {\"performance\": \"ALL\"})\n    service = Service(ChromeDriverManager().install())\n    return webdriver.Chrome(service=service, options=opts)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Step 1: Load the user reviews page and discover its structure ──\n# The user reviews URL uses /reviews?type=user (or sometimes /reviews/user).\n# Run this cell to see what the rendered page looks like and what API calls RT makes.\n\nurl = f\"https://www.rottentomatoes.com/m/{MOVIE_SLUG}/reviews?type=user\"\nprint(f\"Loading: {url}\")\n\ndriver = make_driver(headless=HEADLESS)\ndriver.get(url)\ntime.sleep(5)\n\n# ── Inspect network logs for API endpoints ──\nprint(\"=\" * 60)\nprint(\"NETWORK REQUESTS (looking for review/user-related API calls):\")\nprint(\"=\" * 60)\nlogs = driver.get_log(\"performance\")\napi_urls = set()\nfor entry in logs:\n    try:\n        msg = json.loads(entry[\"message\"])[\"message\"]\n        if msg[\"method\"] == \"Network.requestWillBeSent\":\n            req_url = msg[\"params\"][\"request\"][\"url\"]\n            if any(kw in req_url.lower() for kw in [\"review\", \"napi\", \"user\", \"audience\"]):\n                api_urls.add(req_url)\n                print(f\"  → {req_url}\")\n    except (KeyError, json.JSONDecodeError):\n        pass\nif not api_urls:\n    print(\"  (no review-related API calls detected yet)\")\n\n# ── Inspect the rendered DOM ──\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DOM INSPECTION:\")\nprint(\"=\" * 60)\nsoup = BeautifulSoup(driver.page_source, \"lxml\")\n\n# Check for common review container patterns\nfor cls in [\"review-row\", \"audience-reviews__review\", \"review_table_row\",\n            \"audience-review-row\", \"review-card\"]:\n    found = soup.find_all(attrs={\"class\": lambda c: c and cls in str(c)})\n    if found:\n        print(f\"  .{cls}: {len(found)} elements\")\n\n# data-qa attributes\ndata_qa_elements = soup.find_all(attrs={\"data-qa\": True})\nqa_values = sorted(set(el[\"data-qa\"] for el in data_qa_elements))\nprint(f\"  data-qa attributes: {qa_values}\")\n\n# Custom RT web components\nall_tags = set(tag.name for tag in soup.find_all(True))\nrt_tags = sorted(t for t in all_tags if t and t.startswith(\"rt-\"))\nprint(f\"  Custom rt-* components: {rt_tags}\")\n\n# Classes containing 'review' or 'audience'\nreview_classes = set()\nfor el in soup.find_all(attrs={\"class\": True}):\n    for cls in el.get(\"class\", []):\n        if \"review\" in cls.lower() or \"audience\" in cls.lower():\n            review_classes.add(cls)\nprint(f\"  Classes with 'review'/'audience': {sorted(review_classes)}\")\n\n# Print visible text\nprint(f\"\\n--- First 3000 chars of visible text ---\")\nbody = soup.find(\"body\")\nif body:\n    text = body.get_text(separator=\"\\n\", strip=True)\n    print(text[:3000])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Step 2: Click \"Load More\" and capture network responses ───\n\ndef extract_review_json_from_logs(driver):\n    \"\"\"Extract review data from Chrome performance logs (network responses).\"\"\"\n    results = []\n    logs = driver.get_log(\"performance\")\n    for entry in logs:\n        try:\n            msg = json.loads(entry[\"message\"])[\"message\"]\n            if msg[\"method\"] == \"Network.responseReceived\":\n                resp_url = msg[\"params\"][\"response\"][\"url\"]\n                if any(kw in resp_url.lower() for kw in [\"review\", \"/napi/\", \"user\", \"audience\"]):\n                    request_id = msg[\"params\"][\"requestId\"]\n                    try:\n                        body = driver.execute_cdp_cmd(\n                            \"Network.getResponseBody\",\n                            {\"requestId\": request_id}\n                        )\n                        data = json.loads(body.get(\"body\", \"{}\"))\n                        results.append({\"url\": resp_url, \"data\": data})\n                    except Exception:\n                        pass\n        except (KeyError, json.JSONDecodeError):\n            pass\n    return results\n\ndef find_load_more_button(driver):\n    \"\"\"Find the Load More button using multiple strategies.\"\"\"\n    for selector in [\n        \"button[data-qa='dlp-load-more-button']\",\n        \"rt-button[data-loadmore]\",\n        \"[data-qa='load-more-btn']\",\n        \"button.load-more-button\",\n        \"button.js-load-more-btn\",\n    ]:\n        try:\n            btn = driver.find_element(By.CSS_SELECTOR, selector)\n            if btn.is_displayed():\n                return btn\n        except NoSuchElementException:\n            continue\n\n    # XPath fallback\n    try:\n        btn = driver.find_element(\n            By.XPATH, \"//button[contains(translate(., 'LOADMRE', 'loadmre'), 'load more')]\"\n        )\n        if btn.is_displayed():\n            return btn\n    except NoSuchElementException:\n        pass\n\n    # Iterate all buttons\n    for btn in driver.find_elements(By.TAG_NAME, \"button\"):\n        txt = btn.text.strip().lower()\n        if \"load more\" in txt or \"show more\" in txt:\n            if btn.is_displayed():\n                return btn\n\n    # Check shadow DOM inside rt-button elements\n    try:\n        for rt_btn in driver.find_elements(By.CSS_SELECTOR, \"rt-button\"):\n            shadow = driver.execute_script(\"return arguments[0].shadowRoot\", rt_btn)\n            if shadow:\n                inner_btn = shadow.find_element(By.CSS_SELECTOR, \"button\")\n                if \"load more\" in inner_btn.text.strip().lower():\n                    return rt_btn\n    except Exception:\n        pass\n\n    return None\n\ndef load_all_reviews(driver, max_clicks=500, pause_range=(1.0, 2.5)):\n    \"\"\"Click 'Load More' until all reviews are loaded. Returns captured API data.\"\"\"\n    all_api_data = []\n    clicks = 0\n\n    while clicks < max_clicks:\n        load_more = find_load_more_button(driver)\n        if load_more is None:\n            print(f\"No more 'Load More' button after {clicks} clicks. All reviews loaded.\")\n            break\n\n        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more)\n        time.sleep(0.3)\n        try:\n            load_more.click()\n        except ElementClickInterceptedException:\n            driver.execute_script(\"arguments[0].click();\", load_more)\n\n        clicks += 1\n        if clicks % 10 == 0:\n            print(f\"  Clicked 'Load More' {clicks} times...\")\n\n        time.sleep(random.uniform(*pause_range))\n        all_api_data.extend(extract_review_json_from_logs(driver))\n\n    return clicks, all_api_data\n\ntotal_clicks, captured_api_data = load_all_reviews(driver)\nprint(f\"\\nDone. Clicked 'Load More' {total_clicks} times.\")\nprint(f\"Captured {len(captured_api_data)} API responses containing review data.\")\n\nif captured_api_data:\n    print(\"\\nCaptured API endpoints:\")\n    for item in captured_api_data[:5]:\n        print(f\"  → {item['url']}\")\n        if isinstance(item['data'], dict):\n            print(f\"    Keys: {list(item['data'].keys())[:10]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Step 3: Parse user reviews (API JSON → HTML fallback) ─────\n\ndef parse_user_reviews_from_api(api_data_list):\n    \"\"\"\n    Extract user reviews from captured API JSON responses.\n    User reviews typically have: username, rating/score, review text, date,\n    and sometimes verified/super-reviewer flags.\n    \"\"\"\n    reviews = []\n    seen = set()\n\n    for item in api_data_list:\n        data = item[\"data\"]\n\n        review_lists = []\n        if isinstance(data, list):\n            review_lists.append(data)\n        elif isinstance(data, dict):\n            for key in [\"reviews\", \"items\", \"results\", \"data\", \"rows\", \"userReviews\"]:\n                if key in data and isinstance(data[key], list):\n                    review_lists.append(data[key])\n            for key, val in data.items():\n                if isinstance(val, dict):\n                    for subkey in [\"reviews\", \"items\", \"results\"]:\n                        if subkey in val and isinstance(val[subkey], list):\n                            review_lists.append(val[subkey])\n\n        for review_list in review_lists:\n            for r in review_list:\n                if not isinstance(r, dict):\n                    continue\n\n                # Username\n                username = \"\"\n                user_obj = r.get(\"user\") or r.get(\"author\") or {}\n                if isinstance(user_obj, dict):\n                    username = user_obj.get(\"displayName\") or user_obj.get(\"name\") or user_obj.get(\"username\", \"\")\n                elif isinstance(user_obj, str):\n                    username = user_obj\n                if not username:\n                    username = r.get(\"userName\") or r.get(\"username\") or r.get(\"displayName\", \"\")\n\n                # User ID\n                user_id = \"\"\n                if isinstance(user_obj, dict):\n                    user_id = str(user_obj.get(\"userId\") or user_obj.get(\"id\", \"\"))\n                if not user_id:\n                    user_id = str(r.get(\"userId\") or r.get(\"user_id\", \"\"))\n\n                # Review text\n                review_text = (\n                    r.get(\"review\") or r.get(\"reviewText\") or r.get(\"text\")\n                    or r.get(\"quote\") or r.get(\"comment\") or \"\"\n                )\n\n                # Date\n                date = (\n                    r.get(\"createDate\") or r.get(\"creationDate\") or r.get(\"date\")\n                    or r.get(\"reviewDate\") or r.get(\"submittedDate\") or \"\"\n                )\n\n                # Rating (user reviews use star ratings, typically 0.5 - 5.0)\n                rating = r.get(\"rating\") or r.get(\"score\") or r.get(\"stars\") or \"\"\n                if isinstance(rating, dict):\n                    rating = rating.get(\"value\") or rating.get(\"score\", \"\")\n\n                # Verified / super reviewer flags\n                is_verified = r.get(\"isVerified\", False) or r.get(\"verified\", False)\n                is_super_reviewer = r.get(\"isSuperReviewer\", False)\n                has_spoilers = r.get(\"hasSpoilers\", False) or r.get(\"isSpoiler\", False)\n\n                # De-duplicate\n                dedup_key = (str(username), str(review_text)[:50])\n                if dedup_key in seen or (not review_text and not username):\n                    continue\n                seen.add(dedup_key)\n\n                reviews.append({\n                    \"Username\": str(username).strip(),\n                    \"User ID\": str(user_id).strip(),\n                    \"Date\": str(date).strip(),\n                    \"User Review\": str(review_text).strip(),\n                    \"Rating\": rating if rating != \"\" else \"NA\",\n                    \"Verified\": is_verified,\n                    \"Super Reviewer\": is_super_reviewer,\n                    \"Has Spoilers\": has_spoilers,\n                })\n\n    return reviews\n\n\ndef parse_user_reviews_from_html(driver):\n    \"\"\"\n    Fallback: parse user reviews from rendered HTML.\n    Uses multiple strategies for RT's evolving page structure.\n    \"\"\"\n    soup = BeautifulSoup(driver.page_source, \"lxml\")\n    reviews = []\n\n    # ── Strategy A: div.review-row or audience-review-row ──\n    rows = soup.find_all(\"div\", class_=\"review-row\")\n    if not rows:\n        rows = soup.find_all(\"div\", class_=lambda c: c and \"audience\" in c and \"review\" in c)\n    if rows:\n        print(f\"  HTML Strategy A: Found {len(rows)} review row elements\")\n        for row in rows:\n            review = {}\n\n            # Username\n            for el in [\n                row.find(\"a\", attrs={\"data-qa\": lambda v: v and \"user\" in v.lower()}),\n                row.find(\"a\", class_=lambda c: c and \"user\" in c.lower()),\n                row.find(\"rt-link\", attrs={\"slot\": lambda v: v and \"user\" in v.lower()}),\n                row.find(\"a\", href=lambda h: h and \"/user/\" in h),\n            ]:\n                if el:\n                    review[\"Username\"] = el.get_text(strip=True)\n                    href = el.get(\"href\", \"\")\n                    review[\"User ID\"] = href.split(\"/user/\")[-1].strip(\"/\") if \"/user/\" in href else \"\"\n                    break\n            else:\n                review[\"Username\"] = \"\"\n                review[\"User ID\"] = \"\"\n\n            # Date\n            for el in [\n                row.find(attrs={\"data-qa\": lambda v: v and \"date\" in v.lower()}),\n                row.find(\"rt-text\", attrs={\"slot\": lambda v: v and \"date\" in v.lower()}),\n                row.find(\"span\", class_=lambda c: c and \"date\" in c.lower()),\n            ]:\n                if el:\n                    review[\"Date\"] = el.get_text(strip=True)\n                    break\n            else:\n                review[\"Date\"] = \"\"\n\n            # Review text\n            for el in [\n                row.find(attrs={\"data-qa\": lambda v: v and \"review\" in v.lower() and \"text\" in v.lower()}),\n                row.find(\"p\", class_=lambda c: c and \"review\" in c.lower()),\n                row.find(\"div\", class_=lambda c: c and \"review\" in c.lower() and \"text\" in c.lower()),\n                row.find(\"rt-text\", attrs={\"slot\": lambda v: v and \"review\" in v.lower()}),\n            ]:\n                if el:\n                    review[\"User Review\"] = el.get_text(strip=True)\n                    break\n            else:\n                review[\"User Review\"] = \"\"\n\n            # Rating — look for star elements or score displays\n            rating = \"NA\"\n            star_el = row.find(attrs={\"class\": lambda c: c and \"star\" in c.lower()})\n            if star_el:\n                filled = row.find_all(attrs={\"class\": lambda c: c and \"filled\" in c.lower()})\n                half = row.find_all(attrs={\"class\": lambda c: c and \"half\" in c.lower()})\n                rating = len(filled) + 0.5 * len(half) if filled or half else \"NA\"\n            score_el = row.find(attrs={\"data-qa\": lambda v: v and \"score\" in v.lower()})\n            if score_el and rating == \"NA\":\n                try:\n                    rating = float(score_el.get_text(strip=True).split(\"/\")[0])\n                except (ValueError, IndexError):\n                    pass\n            review[\"Rating\"] = rating\n\n            review[\"Verified\"] = False\n            review[\"Super Reviewer\"] = False\n            review[\"Has Spoilers\"] = False\n            reviews.append(review)\n\n        return reviews\n\n    # ── Strategy B: data-qa based ──\n    rows = soup.find_all(attrs={\"data-qa\": lambda v: v and \"review\" in v.lower()})\n    if rows:\n        print(f\"  HTML Strategy B: Found {len(rows)} data-qa review elements\")\n        for row in rows:\n            texts = [t.strip() for t in row.stripped_strings if t.strip()]\n            reviews.append({\n                \"Username\": texts[0] if len(texts) > 0 else \"\",\n                \"User ID\": \"\",\n                \"Date\": texts[-1] if len(texts) > 1 else \"\",\n                \"User Review\": \" \".join(texts[1:-1]) if len(texts) > 2 else \"\",\n                \"Rating\": \"NA\",\n                \"Verified\": False,\n                \"Super Reviewer\": False,\n                \"Has Spoilers\": False,\n            })\n        return reviews\n\n    # ── Strategy C: JavaScript DOM access ──\n    try:\n        js_reviews = driver.execute_script(\"\"\"\n            const rows = document.querySelectorAll(\n                'div[class*=\"review\"], [data-qa*=\"review\"], .review-row'\n            );\n            return Array.from(rows).map(r => r.innerText);\n        \"\"\")\n        if js_reviews:\n            print(f\"  HTML Strategy C (JS): Found {len(js_reviews)} text blocks\")\n            for text in js_reviews:\n                lines = [l.strip() for l in text.split('\\n') if l.strip()]\n                reviews.append({\n                    \"Username\": lines[0] if len(lines) > 0 else \"\",\n                    \"User ID\": \"\",\n                    \"Date\": \"\",\n                    \"User Review\": \" \".join(lines[1:]) if len(lines) > 1 else \"\",\n                    \"Rating\": \"NA\",\n                    \"Verified\": False,\n                    \"Super Reviewer\": False,\n                    \"Has Spoilers\": False,\n                })\n            return reviews\n    except Exception:\n        pass\n\n    print(\"WARNING: No reviews found with any strategy.\")\n    print(\"Set HEADLESS = False, re-run Step 1, and inspect the page in the browser.\")\n    return reviews\n\n\n# ── Try API data first, fall back to HTML ──\nreviews = []\nif captured_api_data:\n    reviews = parse_user_reviews_from_api(captured_api_data)\n    print(f\"Parsed {len(reviews)} user reviews from captured API responses.\")\n\nif not reviews:\n    print(\"API parsing yielded 0 reviews. Falling back to HTML parsing...\")\n    reviews = parse_user_reviews_from_html(driver)\n    print(f\"Parsed {len(reviews)} user reviews from HTML.\")\n\n# Add review IDs\nfor i, r in enumerate(reviews, start=1):\n    r[\"Review ID\"] = i\n\nprint(f\"\\nTotal user reviews: {len(reviews)}\")\nif reviews:\n    print(f\"\\nSample review:\")\n    for k, v in reviews[0].items():\n        print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Step 4: Save to CSV ───────────────────────────────────────\n\ndf = pd.DataFrame(reviews)\n\n# Reorder columns\ncol_order = [\"Review ID\", \"Username\", \"User ID\", \"Date\", \"User Review\",\n             \"Rating\", \"Verified\", \"Super Reviewer\", \"Has Spoilers\"]\ndf = df[[c for c in col_order if c in df.columns]]\n\ncsv_filename = f\"{MOVIE_SLUG}_user_reviews.csv\"\ncsv_path = os.path.join(OUTPUT_DIR, csv_filename)\ndf.to_csv(csv_path, index=False, encoding=\"utf-8\")\nprint(f\"Saved {len(df)} reviews to: {csv_path}\")\nprint(f\"\\nColumn summary:\")\nfor col in df.columns:\n    non_empty = df[col].astype(str).str.strip().ne(\"\").sum()\n    print(f\"  {col}: {non_empty}/{len(df)} non-empty\")\ndf.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Step 5: Clean up ──────────────────────────────────────────\n\ndriver.quit()\nprint(\"Browser closed.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}