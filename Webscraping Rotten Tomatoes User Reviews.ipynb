{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen\n",
    "from urllib2 import HTTPError, URLError\n",
    "import re\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import unicodecsv as csv\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    hdr = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    return BeautifulSoup(urllib2.urlopen(urllib2.Request(url,headers=hdr)).read(),'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_review(url,startID):\n",
    "    soup = make_soup(url)\n",
    "    divtable = soup.find(\"div\", class_ = \"review_table\")\n",
    "    reviewlist = divtable.find_all(\"div\", class_ = \"row review_table_row\")\n",
    "    completereviewdict = {}\n",
    "    for x in reviewlist: \n",
    "        completereviewdict[str(startID)] = {}\n",
    "        leftcolumn = x.find(\"div\", class_ = \"col-xs-8\")\n",
    "        username = leftcolumn.get_text().strip()\n",
    "        userid = leftcolumn.find(\"a\", class_ = \"bold unstyled articleLink\")[\"href\"].split(\"/\")[3]\n",
    "        rightcolumn = x.find(\"div\", class_ = \"col-xs-16\")\n",
    "        date = rightcolumn.find(\"span\", class_ = \"fr small subtle\").get_text()\n",
    "        userreview = rightcolumn.find(\"div\", class_ = \"user_review\").get_text().strip()\n",
    "        if len(rightcolumn.find_all(\"span\",class_=\"fl\")) > 0:\n",
    "            starlist = rightcolumn.find_all(\"span\", class_ = \"glyphicon glyphicon-star\")\n",
    "            if len(starlist) > 0:\n",
    "                countstar = len(starlist)\n",
    "                if rightcolumn.find_all(\"span\",class_=\"fl\")[0].get_text() != ' ':\n",
    "                    rating = float(countstar) + 0.5\n",
    "                else:\n",
    "                    rating = float(countstar)\n",
    "            else:\n",
    "                rating = 0.5\n",
    "        else:\n",
    "            rating = \"NA\"\n",
    "        reviewdict = {\"Username\":username, \"User ID\": userid, \"Date\":date, \"User Review\":userreview,\"Rating\":rating}\n",
    "        completereviewdict[str(startID)].update(reviewdict)\n",
    "        startID += 1\n",
    "    return completereviewdict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 processed.\n",
      "Page 2 processed.\n",
      "Page 3 processed.\n",
      "Page 4 processed.\n",
      "Page 5 processed.\n",
      "Page 6 processed.\n",
      "Page 7 processed.\n",
      "Page 8 processed.\n",
      "Page 9 processed.\n",
      "Page 10 processed.\n",
      "Page 11 processed.\n",
      "Page 12 processed.\n",
      "Page 13 processed.\n",
      "Page 14 processed.\n",
      "Page 15 processed.\n",
      "Page 16 processed.\n",
      "Page 17 processed.\n",
      "Page 18 processed.\n",
      "Page 19 processed.\n",
      "Page 20 processed.\n",
      "Page 21 processed.\n",
      "Page 22 processed.\n",
      "Page 23 processed.\n",
      "Page 24 processed.\n",
      "Page 25 processed.\n",
      "Page 26 processed.\n",
      "Page 27 processed.\n",
      "Page 28 processed.\n",
      "Page 29 processed.\n",
      "Page 30 processed.\n",
      "Page 31 processed.\n",
      "Page 32 processed.\n",
      "Page 33 processed.\n",
      "Page 34 processed.\n",
      "Page 35 processed.\n",
      "Page 36 processed.\n",
      "Page 37 processed.\n",
      "Page 38 processed.\n",
      "Page 39 processed.\n",
      "Page 40 processed.\n",
      "Page 41 processed.\n",
      "Page 42 processed.\n",
      "Page 43 processed.\n",
      "Page 44 processed.\n",
      "Page 45 processed.\n",
      "Page 46 processed.\n",
      "Page 47 processed.\n",
      "Page 48 processed.\n",
      "Page 49 processed.\n",
      "Page 50 processed.\n",
      "Page 51 processed.\n"
     ]
    }
   ],
   "source": [
    "bigdict = {}\n",
    "for x in range(1,52): # Change ending index (exclusive) accordingly\n",
    "    url = \"https://www.rottentomatoes.com/m/hurt_locker/reviews/?page=\"+str(x)+\"&type=user\" # Change movie url accordingly\n",
    "    pageStartID = (x-1)*20 + 1\n",
    "    try:\n",
    "        onePageDict = get_review(url,pageStartID)\n",
    "        print 'Page ' + str(x) + ' processed.'\n",
    "    except AttributeError:\n",
    "        print 'Page ' + str(x) + ' not processed.'\n",
    "        continue\n",
    "    bigdict.update(onePageDict)\n",
    "    rdtime = random.uniform(0.5,2)\n",
    "    sleep(rdtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1012"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigdict) # Check how many reviews were scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/lulinghuang/Desktop/2017 DSC/sci-fi/scrape_rottentmt/\") # Change directory accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing file.\n"
     ]
    }
   ],
   "source": [
    "with open(\"test_HurtLocker_1To51_April26.csv\", \"w\") as toWrite: # Change file name accordingly\n",
    "    writer = csv.writer(toWrite, delimiter=',')\n",
    "    writer.writerow([\"Review ID\", \"Date\", \n",
    "    'User ID', 'User Review', 'Username','Rating'])\n",
    "    for a in bigdict.keys():\n",
    "        try:\n",
    "            writer.writerow([a,\n",
    "                            bigdict[a][\"Date\"].encode('utf-8'),\n",
    "                            bigdict[a][\"User ID\"].encode('utf-8'),\n",
    "                            bigdict[a][\"User Review\"].encode('utf-8'),\n",
    "                            bigdict[a][\"Username\"].encode('utf-8'),\n",
    "                            str(bigdict[a][\"Rating\"]).encode('utf-8')])\n",
    "        except UnicodeDecodeError as detail:\n",
    "            print 'UnicodeDecodeError'\n",
    "            print a\n",
    "            print detail\n",
    "            continue\n",
    "        except AttributeError as detail:\n",
    "            print 'AttributeError'\n",
    "            print a\n",
    "            print detail\n",
    "            continue\n",
    "print 'Done writing file.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creat a txt for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_HurtLocker_1To51_April26.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index,row in df.iterrows():\n",
    "    date = row['Date']\n",
    "    d = datetime.datetime.strptime(date, '%B %d, %Y')\n",
    "    newd = d.strftime('%Y-%m-%d')\n",
    "    reviewid = row['Review ID']\n",
    "    moview = 'HL'\n",
    "    filename = moview + '_' + newd + '_' + str(reviewid) + '.txt'\n",
    "    outpath = os.path.join(\"/Users/lulinghuang/Desktop/2017 DSC/sci-fi/scrape_rottentmt/HL_1to51_txt/\", filename)\n",
    "    with open(outpath,'w') as f:\n",
    "        f.write(row['User Review'].encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
