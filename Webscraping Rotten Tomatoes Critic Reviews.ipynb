{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# Webscraping Rotten Tomatoes Critic Reviews (Python 3 + Selenium)\n# Updated 2026 to work with RT's current JavaScript-rendered site.\n#\n# Strategy:\n#   1. Use Selenium to load the reviews page and click \"Load More\"\n#   2. Intercept the internal API (XHR/fetch) responses for clean JSON data\n#   3. Fall back to HTML parsing if API interception doesn't work\n#\n# Requirements:\n#   pip install selenium beautifulsoup4 pandas lxml webdriver-manager\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import (\n    TimeoutException, NoSuchElementException, ElementClickInterceptedException\n)\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport json\nimport time\nimport random\nimport re\nimport os"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Configuration ──────────────────────────────────────────────\n# Change this slug to scrape a different movie.\n# The slug is the part after /m/ in the RT URL, e.g.:\n#   https://www.rottentomatoes.com/m/the_hurt_locker  →  \"the_hurt_locker\"\n\nMOVIE_SLUG = \"the_hurt_locker\"\n\n# Where to save the output CSV (defaults to this notebook's directory)\nOUTPUT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n\n# Set to True to run Chrome visibly (useful for debugging selectors)\nHEADLESS = True"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Helper: create a Selenium Chrome driver ───────────────────\n\ndef make_driver(headless=True, enable_network_logging=True):\n    \"\"\"Create and return a Chrome WebDriver with optional network logging.\"\"\"\n    opts = Options()\n    if headless:\n        opts.add_argument(\"--headless=new\")\n    opts.add_argument(\"--no-sandbox\")\n    opts.add_argument(\"--disable-dev-shm-usage\")\n    opts.add_argument(\"--disable-gpu\")\n    opts.add_argument(\"--window-size=1920,1080\")\n    opts.add_argument(\n        \"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n    )\n\n    # Enable performance logging to capture network requests/responses\n    if enable_network_logging:\n        opts.set_capability(\"goog:loggingPrefs\", {\"performance\": \"ALL\"})\n\n    service = Service(ChromeDriverManager().install())\n    return webdriver.Chrome(service=service, options=opts)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Step 1: Load the page and discover its structure ──────────\n# This cell loads the reviews page, waits for JS rendering, and then\n# inspects both the DOM structure and the network requests RT makes.\n# Use this to debug selectors if the scraper breaks after an RT update.\n\nurl = f\"https://www.rottentomatoes.com/m/{MOVIE_SLUG}/reviews\"\nprint(f\"Loading: {url}\")\n\ndriver = make_driver(headless=HEADLESS)\ndriver.get(url)\ntime.sleep(5)  # wait for JS to render\n\n# ── Inspect network logs for API endpoints ──\nprint(\"=\" * 60)\nprint(\"NETWORK REQUESTS (looking for review-related API calls):\")\nprint(\"=\" * 60)\nlogs = driver.get_log(\"performance\")\napi_urls = set()\nfor entry in logs:\n    try:\n        msg = json.loads(entry[\"message\"])[\"message\"]\n        if msg[\"method\"] == \"Network.requestWillBeSent\":\n            req_url = msg[\"params\"][\"request\"][\"url\"]\n            if any(kw in req_url.lower() for kw in [\"review\", \"napi\", \"critic\"]):\n                api_urls.add(req_url)\n                print(f\"  → {req_url}\")\n    except (KeyError, json.JSONDecodeError):\n        pass\nif not api_urls:\n    print(\"  (no review-related API calls detected yet)\")\n\n# ── Inspect the rendered DOM ──\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DOM INSPECTION:\")\nprint(\"=\" * 60)\nsoup = BeautifulSoup(driver.page_source, \"lxml\")\n\n# Check for review-row divs\nreview_rows = soup.find_all(\"div\", class_=\"review-row\")\nprint(f\"  div.review-row: {len(review_rows)}\")\n\n# Check for data-qa attributes\ndata_qa_elements = soup.find_all(attrs={\"data-qa\": True})\nqa_values = sorted(set(el[\"data-qa\"] for el in data_qa_elements))\nprint(f\"  data-qa attributes: {qa_values}\")\n\n# Check for custom RT web components\nall_tags = set(tag.name for tag in soup.find_all(True))\nrt_tags = sorted(t for t in all_tags if t and t.startswith(\"rt-\"))\nprint(f\"  Custom rt-* components: {rt_tags}\")\n\n# Look for any element with 'review' in class name\nreview_classes = set()\nfor el in soup.find_all(attrs={\"class\": True}):\n    for cls in el.get(\"class\", []):\n        if \"review\" in cls.lower():\n            review_classes.add(cls)\nprint(f\"  Classes containing 'review': {sorted(review_classes)}\")\n\n# Print first chunk of visible text for sanity check\nprint(f\"\\n--- First 3000 chars of visible text ---\")\nbody = soup.find(\"body\")\nif body:\n    text = body.get_text(separator=\"\\n\", strip=True)\n    print(text[:3000])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Step 2: Click \"Load More\" and capture network responses ───\n# RT loads reviews via internal API calls (XHR/fetch). Each time we\n# click \"Load More\", new review data is fetched. We capture both\n# the API JSON responses AND keep the final rendered HTML as fallback.\n\ndef extract_review_json_from_logs(driver):\n    \"\"\"Extract review data from Chrome performance logs (network responses).\"\"\"\n    reviews_from_api = []\n    logs = driver.get_log(\"performance\")\n    for entry in logs:\n        try:\n            msg = json.loads(entry[\"message\"])[\"message\"]\n            if msg[\"method\"] == \"Network.responseReceived\":\n                resp_url = msg[\"params\"][\"response\"][\"url\"]\n                if any(kw in resp_url.lower() for kw in [\"review\", \"/napi/\"]):\n                    request_id = msg[\"params\"][\"requestId\"]\n                    try:\n                        body = driver.execute_cdp_cmd(\n                            \"Network.getResponseBody\",\n                            {\"requestId\": request_id}\n                        )\n                        data = json.loads(body.get(\"body\", \"{}\"))\n                        reviews_from_api.append({\n                            \"url\": resp_url,\n                            \"data\": data\n                        })\n                    except Exception:\n                        pass\n        except (KeyError, json.JSONDecodeError):\n            pass\n    return reviews_from_api\n\ndef find_load_more_button(driver):\n    \"\"\"Find the Load More button using multiple selector strategies.\"\"\"\n    # Try CSS selectors first\n    for selector in [\n        \"button[data-qa='dlp-load-more-button']\",\n        \"rt-button[data-loadmore]\",\n        \"[data-qa='load-more-btn']\",\n        \"button.load-more-button\",\n        \"button.js-load-more-btn\",\n    ]:\n        try:\n            btn = driver.find_element(By.CSS_SELECTOR, selector)\n            if btn.is_displayed():\n                return btn\n        except NoSuchElementException:\n            continue\n\n    # Try XPath for buttons containing \"Load More\" text\n    try:\n        btn = driver.find_element(\n            By.XPATH, \"//button[contains(translate(., 'LOADMRE', 'loadmre'), 'load more')]\"\n        )\n        if btn.is_displayed():\n            return btn\n    except NoSuchElementException:\n        pass\n\n    # Fallback: iterate all buttons\n    for btn in driver.find_elements(By.TAG_NAME, \"button\"):\n        txt = btn.text.strip().lower()\n        if \"load more\" in txt or \"show more\" in txt:\n            if btn.is_displayed():\n                return btn\n\n    # Also check inside shadow DOM of rt-button elements\n    try:\n        rt_buttons = driver.find_elements(By.CSS_SELECTOR, \"rt-button\")\n        for rt_btn in rt_buttons:\n            shadow = driver.execute_script(\"return arguments[0].shadowRoot\", rt_btn)\n            if shadow:\n                inner_btn = shadow.find_element(By.CSS_SELECTOR, \"button\")\n                if \"load more\" in inner_btn.text.strip().lower():\n                    return rt_btn  # click the outer rt-button\n    except Exception:\n        pass\n\n    return None\n\ndef load_all_reviews(driver, max_clicks=200, pause_range=(1.0, 2.5)):\n    \"\"\"Click 'Load More' until all reviews are loaded. Returns captured API data.\"\"\"\n    all_api_data = []\n    clicks = 0\n\n    while clicks < max_clicks:\n        load_more = find_load_more_button(driver)\n\n        if load_more is None:\n            print(f\"No more 'Load More' button after {clicks} clicks. All reviews loaded.\")\n            break\n\n        # Scroll to button and click\n        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more)\n        time.sleep(0.3)\n        try:\n            load_more.click()\n        except ElementClickInterceptedException:\n            driver.execute_script(\"arguments[0].click();\", load_more)\n\n        clicks += 1\n        if clicks % 10 == 0:\n            print(f\"  Clicked 'Load More' {clicks} times...\")\n\n        time.sleep(random.uniform(*pause_range))\n\n        # Capture any API responses from this click\n        api_data = extract_review_json_from_logs(driver)\n        all_api_data.extend(api_data)\n\n    return clicks, all_api_data\n\ntotal_clicks, captured_api_data = load_all_reviews(driver)\nprint(f\"\\nDone. Clicked 'Load More' {total_clicks} times.\")\nprint(f\"Captured {len(captured_api_data)} API responses containing review data.\")\n\n# Show what API URLs were captured\nif captured_api_data:\n    print(\"\\nCaptured API endpoints:\")\n    for item in captured_api_data[:5]:\n        print(f\"  → {item['url']}\")\n        if isinstance(item['data'], dict):\n            print(f\"    Keys: {list(item['data'].keys())[:10]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Step 3: Parse reviews (API JSON → HTML fallback) ──────────\n\ndef parse_reviews_from_api(api_data_list):\n    \"\"\"\n    Try to extract structured reviews from captured API JSON responses.\n    RT's internal API typically returns reviews with fields like:\n    critic, publication, quote/review, date, sentiment, score, etc.\n    \n    Since RT may change field names, this inspects the JSON structure\n    and tries common patterns.\n    \"\"\"\n    reviews = []\n    seen = set()\n\n    for item in api_data_list:\n        data = item[\"data\"]\n\n        # The API response might have reviews at the top level or nested\n        review_lists = []\n        if isinstance(data, list):\n            review_lists.append(data)\n        elif isinstance(data, dict):\n            # Check common keys where reviews might be stored\n            for key in [\"reviews\", \"items\", \"results\", \"data\", \"critics\", \"rows\"]:\n                if key in data and isinstance(data[key], list):\n                    review_lists.append(data[key])\n            # Also check nested: data.reviews, data.pageInfo, etc.\n            for key, val in data.items():\n                if isinstance(val, dict):\n                    for subkey in [\"reviews\", \"items\", \"results\"]:\n                        if subkey in val and isinstance(val[subkey], list):\n                            review_lists.append(val[subkey])\n\n        for review_list in review_lists:\n            for r in review_list:\n                if not isinstance(r, dict):\n                    continue\n\n                # Extract fields using common RT API field names\n                critic = (\n                    r.get(\"criticName\") or r.get(\"critic\", {}).get(\"name\", \"\")\n                    if isinstance(r.get(\"critic\"), dict) else r.get(\"critic\", \"\")\n                )\n                publication = (\n                    r.get(\"publicationName\") or r.get(\"publication\", {}).get(\"name\", \"\")\n                    if isinstance(r.get(\"publication\"), dict) else r.get(\"publication\", \"\")\n                )\n                review_text = (\n                    r.get(\"quote\") or r.get(\"reviewText\") or r.get(\"text\")\n                    or r.get(\"review\") or \"\"\n                )\n                date = (\n                    r.get(\"creationDate\") or r.get(\"date\") or r.get(\"reviewDate\")\n                    or r.get(\"publicationDate\") or \"\"\n                )\n                sentiment = r.get(\"sentiment\") or r.get(\"tomatometerState\") or \"\"\n                if not sentiment:\n                    score_val = r.get(\"score\") or r.get(\"rating\") or r.get(\"originalScore\")\n                    if isinstance(score_val, str) and (\"fresh\" in score_val.lower()):\n                        sentiment = \"Fresh\"\n                    elif isinstance(score_val, str) and (\"rotten\" in score_val.lower()):\n                        sentiment = \"Rotten\"\n                    # Check for isFresh / isRotten boolean flags\n                    if r.get(\"isFresh\") or r.get(\"isPositive\"):\n                        sentiment = \"Fresh\"\n                    elif r.get(\"isRotten\") or r.get(\"isNegative\"):\n                        sentiment = \"Rotten\"\n\n                original_score = r.get(\"originalScore\") or r.get(\"score\") or \"\"\n                review_url = r.get(\"url\") or r.get(\"reviewUrl\") or r.get(\"link\") or \"\"\n                is_top_critic = r.get(\"isTopCritic\", False)\n\n                # De-duplicate by (critic, review_text[:50])\n                dedup_key = (str(critic), str(review_text)[:50])\n                if dedup_key in seen:\n                    continue\n                seen.add(dedup_key)\n\n                reviews.append({\n                    \"Critic\": str(critic).strip(),\n                    \"Publication\": str(publication).strip(),\n                    \"Date\": str(date).strip(),\n                    \"Review Text\": str(review_text).strip(),\n                    \"Sentiment\": str(sentiment).strip(),\n                    \"Original Score\": str(original_score).strip(),\n                    \"Review URL\": str(review_url).strip(),\n                    \"Top Critic\": is_top_critic,\n                })\n\n    return reviews\n\n\ndef parse_reviews_from_html(driver):\n    \"\"\"\n    Fallback: parse reviews directly from the rendered HTML/DOM.\n    Uses multiple strategies to handle RT's evolving page structure.\n    \"\"\"\n    soup = BeautifulSoup(driver.page_source, \"lxml\")\n    reviews = []\n\n    # ── Strategy A: div.review-row ──\n    rows = soup.find_all(\"div\", class_=\"review-row\")\n    if rows:\n        print(f\"  HTML Strategy A: Found {len(rows)} 'review-row' elements\")\n        for row in rows:\n            review = {}\n\n            # Critic name — try several selectors\n            for el in [\n                row.find(\"a\", attrs={\"data-qa\": \"review-critic-link\"}),\n                row.find(\"a\", class_=lambda c: c and \"critic\" in c.lower()),\n                row.find(\"rt-link\", attrs={\"slot\": \"criticName\"}),\n                row.find(\"a\", href=lambda h: h and \"/critic/\" in h),\n            ]:\n                if el:\n                    review[\"Critic\"] = el.get_text(strip=True)\n                    break\n            else:\n                review[\"Critic\"] = \"\"\n\n            # Publication\n            for el in [\n                row.find(\"a\", class_=lambda c: c and \"publication\" in c.lower()),\n                row.find(\"rt-link\", attrs={\"slot\": \"publicationName\"}),\n                row.find(\"em\"),\n                row.find(\"a\", href=lambda h: h and \"/source/\" in h),\n            ]:\n                if el:\n                    review[\"Publication\"] = el.get_text(strip=True)\n                    break\n            else:\n                review[\"Publication\"] = \"\"\n\n            # Date\n            for el in [\n                row.find(attrs={\"data-qa\": \"review-date\"}),\n                row.find(\"rt-text\", attrs={\"slot\": \"displayDate\"}),\n                row.find(\"span\", class_=lambda c: c and \"date\" in c.lower()),\n            ]:\n                if el:\n                    review[\"Date\"] = el.get_text(strip=True)\n                    break\n            else:\n                review[\"Date\"] = \"\"\n\n            # Review text\n            for el in [\n                row.find(attrs={\"data-qa\": \"review-text\"}),\n                row.find(\"rt-text\", attrs={\"slot\": \"reviewQuote\"}),\n                row.find(\"p\", class_=lambda c: c and \"review\" in c.lower()),\n                row.find(\"div\", class_=lambda c: c and \"review-text\" in c.lower()),\n            ]:\n                if el:\n                    review[\"Review Text\"] = el.get_text(strip=True)\n                    break\n            else:\n                review[\"Review Text\"] = \"\"\n\n            # Sentiment\n            row_html = str(row).lower()\n            if \"certified-fresh\" in row_html or \"certified_fresh\" in row_html:\n                review[\"Sentiment\"] = \"Certified Fresh\"\n            elif \"fresh\" in row_html and \"rotten\" not in row_html:\n                review[\"Sentiment\"] = \"Fresh\"\n            elif \"rotten\" in row_html:\n                review[\"Sentiment\"] = \"Rotten\"\n            else:\n                review[\"Sentiment\"] = \"\"\n\n            review[\"Original Score\"] = \"\"\n            review[\"Review URL\"] = \"\"\n            review[\"Top Critic\"] = False\n            reviews.append(review)\n\n        return reviews\n\n    # ── Strategy B: data-qa based ──\n    rows = soup.find_all(attrs={\"data-qa\": lambda v: v and \"review\" in v.lower()})\n    if rows:\n        print(f\"  HTML Strategy B: Found {len(rows)} data-qa review elements\")\n        for row in rows:\n            texts = [t.strip() for t in row.stripped_strings if t.strip()]\n            reviews.append({\n                \"Critic\": texts[0] if len(texts) > 0 else \"\",\n                \"Publication\": texts[1] if len(texts) > 1 else \"\",\n                \"Date\": texts[-1] if len(texts) > 2 else \"\",\n                \"Review Text\": \" \".join(texts[2:-1]) if len(texts) > 3 else \"\",\n                \"Sentiment\": \"\",\n                \"Original Score\": \"\",\n                \"Review URL\": \"\",\n                \"Top Critic\": False,\n            })\n        return reviews\n\n    # ── Strategy C: use JavaScript to access shadow DOM ──\n    try:\n        js_reviews = driver.execute_script(\"\"\"\n            const rows = document.querySelectorAll(\n                'div[class*=\"review\"], [data-qa*=\"review\"], .review-row'\n            );\n            return Array.from(rows).map(r => r.innerText);\n        \"\"\")\n        if js_reviews:\n            print(f\"  HTML Strategy C (JS): Found {len(js_reviews)} text blocks\")\n            for text in js_reviews:\n                lines = [l.strip() for l in text.split('\\n') if l.strip()]\n                reviews.append({\n                    \"Critic\": lines[0] if len(lines) > 0 else \"\",\n                    \"Publication\": lines[1] if len(lines) > 1 else \"\",\n                    \"Date\": lines[-1] if len(lines) > 2 else \"\",\n                    \"Review Text\": \" \".join(lines[2:-1]) if len(lines) > 3 else \"\",\n                    \"Sentiment\": \"\",\n                    \"Original Score\": \"\",\n                    \"Review URL\": \"\",\n                    \"Top Critic\": False,\n                })\n            return reviews\n    except Exception:\n        pass\n\n    print(\"WARNING: No reviews found with any strategy.\")\n    print(\"Set HEADLESS = False, re-run Step 1, and inspect the page in the browser.\")\n    return reviews\n\n\n# ── Try API data first, fall back to HTML ──\nreviews = []\nif captured_api_data:\n    reviews = parse_reviews_from_api(captured_api_data)\n    print(f\"Parsed {len(reviews)} reviews from captured API responses.\")\n\nif not reviews:\n    print(\"API parsing yielded 0 reviews. Falling back to HTML parsing...\")\n    reviews = parse_reviews_from_html(driver)\n    print(f\"Parsed {len(reviews)} reviews from HTML.\")\n\n# Add review IDs\nfor i, r in enumerate(reviews, start=1):\n    r[\"Review ID\"] = i\n\nprint(f\"\\nTotal reviews: {len(reviews)}\")\nif reviews:\n    print(f\"\\nSample review:\")\n    for k, v in reviews[0].items():\n        print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Step 4: Save to CSV ───────────────────────────────────────\n\ndf = pd.DataFrame(reviews)\n\n# Reorder columns\ncol_order = [\"Review ID\", \"Critic\", \"Publication\", \"Date\", \"Review Text\",\n             \"Sentiment\", \"Original Score\", \"Top Critic\", \"Review URL\"]\ndf = df[[c for c in col_order if c in df.columns]]\n\ncsv_filename = f\"{MOVIE_SLUG}_critic_reviews.csv\"\ncsv_path = os.path.join(OUTPUT_DIR, csv_filename)\ndf.to_csv(csv_path, index=False, encoding=\"utf-8\")\nprint(f\"Saved {len(df)} reviews to: {csv_path}\")\nprint(f\"\\nColumn summary:\")\nfor col in df.columns:\n    non_empty = df[col].astype(str).str.strip().ne(\"\").sum()\n    print(f\"  {col}: {non_empty}/{len(df)} non-empty\")\ndf.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Step 5: Clean up ──────────────────────────────────────────\n\ndriver.quit()\nprint(\"Browser closed.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}