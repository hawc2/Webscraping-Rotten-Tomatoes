{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# Webscraping Rotten Tomatoes Critic Reviews (Python 3 + Selenium)\n# Updated 2026 to work with RT's current JavaScript-rendered site.\n#\n# Requirements:\n#   pip install selenium beautifulsoup4 pandas lxml webdriver-manager\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import (\n    TimeoutException, NoSuchElementException, ElementClickInterceptedException\n)\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nimport random\nimport os"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Configuration ──────────────────────────────────────────────\n# Change this slug to scrape a different movie.\n# The slug is the part after /m/ in the RT URL, e.g.:\n#   https://www.rottentomatoes.com/m/the_hurt_locker  →  \"the_hurt_locker\"\n\nMOVIE_SLUG = \"the_hurt_locker\"\n\n# Where to save the output CSV (defaults to this notebook's directory)\nOUTPUT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n\n# Set to True to run Chrome visibly (useful for debugging selectors)\nHEADLESS = True"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Helper: create a Selenium Chrome driver ───────────────────\n\ndef make_driver(headless=True):\n    \"\"\"Create and return a Chrome WebDriver.\"\"\"\n    opts = Options()\n    if headless:\n        opts.add_argument(\"--headless=new\")\n    opts.add_argument(\"--no-sandbox\")\n    opts.add_argument(\"--disable-dev-shm-usage\")\n    opts.add_argument(\"--disable-gpu\")\n    opts.add_argument(\"--window-size=1920,1080\")\n    opts.add_argument(\n        \"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n    )\n    service = Service(ChromeDriverManager().install())\n    return webdriver.Chrome(service=service, options=opts)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Step 1: Load the page and discover the current HTML structure ──\n# Run this cell first to see what the rendered page looks like.\n# If RT changes their layout, inspect this output to update selectors below.\n\nurl = f\"https://www.rottentomatoes.com/m/{MOVIE_SLUG}/reviews\"\nprint(f\"Loading: {url}\")\n\ndriver = make_driver(headless=HEADLESS)\ndriver.get(url)\n\n# Wait for reviews to load (up to 15 seconds)\ntime.sleep(5)\n\n# Save a snapshot of the rendered page source for inspection\nsoup = BeautifulSoup(driver.page_source, \"lxml\")\n\n# Print a summary of what we find\nreview_rows = soup.find_all(\"div\", class_=\"review-row\")\nprint(f\"Found {len(review_rows)} review rows with class 'review-row'\")\n\n# Try alternative selectors if the above didn't work\nif not review_rows:\n    # Look for any elements containing review-like content\n    for tag in [\"review-row\", \"critic-review\", \"review_table_row\", \"review-card\"]:\n        found = soup.find_all(attrs={\"class\": lambda c: c and tag in c})\n        if found:\n            print(f\"Found {len(found)} elements matching class containing '{tag}'\")\n\n    # Also check for data-qa attributes (RT uses these)\n    data_qa_elements = soup.find_all(attrs={\"data-qa\": True})\n    qa_values = set(el[\"data-qa\"] for el in data_qa_elements)\n    print(f\"\\ndata-qa attributes found: {sorted(qa_values)}\")\n\nprint(\"\\n--- First 5000 chars of page body for inspection ---\")\nbody = soup.find(\"body\")\nprint(body.get_text()[:5000] if body else \"No body found\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Step 2: Click \"Load More\" to get all reviews ─────────────\n# RT uses cursor-based pagination with a \"Load More\" button.\n# This cell clicks it repeatedly until all reviews are loaded.\n\ndef load_all_reviews(driver, max_clicks=200, pause_range=(0.5, 2.0)):\n    \"\"\"Click the 'Load More' button until all reviews are loaded.\"\"\"\n    clicks = 0\n    while clicks < max_clicks:\n        try:\n            # Try common selectors for the Load More button\n            load_more = None\n            for selector in [\n                \"button[data-qa='dlp-load-more-button']\",\n                \"button.load-more-button\",\n                \"rt-button[data-loadmore]\",\n                \"button:has-text('Load More')\",\n            ]:\n                try:\n                    load_more = driver.find_element(By.CSS_SELECTOR, selector)\n                    break\n                except NoSuchElementException:\n                    continue\n\n            # Fallback: find any button with \"Load More\" or \"Show More\" text\n            if load_more is None:\n                buttons = driver.find_elements(By.TAG_NAME, \"button\")\n                for btn in buttons:\n                    if \"load more\" in btn.text.lower() or \"show more\" in btn.text.lower():\n                        load_more = btn\n                        break\n\n            if load_more is None or not load_more.is_displayed():\n                print(f\"No more 'Load More' button found after {clicks} clicks. All reviews loaded.\")\n                break\n\n            # Scroll to the button and click it\n            driver.execute_script(\"arguments[0].scrollIntoView(true);\", load_more)\n            time.sleep(0.3)\n            try:\n                load_more.click()\n            except ElementClickInterceptedException:\n                driver.execute_script(\"arguments[0].click();\", load_more)\n\n            clicks += 1\n            if clicks % 10 == 0:\n                print(f\"  Clicked 'Load More' {clicks} times...\")\n\n            time.sleep(random.uniform(*pause_range))\n\n        except Exception as e:\n            print(f\"Stopped after {clicks} clicks: {e}\")\n            break\n\n    return clicks\n\ntotal_clicks = load_all_reviews(driver)\nprint(f\"Done. Clicked 'Load More' {total_clicks} times total.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Step 3: Parse all critic reviews from the rendered page ───\n\ndef parse_critic_reviews(driver):\n    \"\"\"\n    Parse critic reviews from the fully-rendered page.\n    \n    This function tries multiple selector strategies to be resilient\n    to minor RT layout changes. If it returns 0 reviews, run Step 1\n    again with HEADLESS=False to inspect the page visually.\n    \"\"\"\n    soup = BeautifulSoup(driver.page_source, \"lxml\")\n    reviews = []\n\n    # ── Strategy 1: look for review-row divs ──\n    rows = soup.find_all(\"div\", class_=\"review-row\")\n    if rows:\n        print(f\"Strategy 1: Found {len(rows)} 'review-row' elements\")\n        for i, row in enumerate(rows, start=1):\n            review = {\"Review ID\": i}\n\n            # Critic name\n            critic_el = (\n                row.find(\"a\", class_=lambda c: c and \"critic\" in c.lower())\n                or row.find(\"a\", attrs={\"data-qa\": \"review-critic-link\"})\n                or row.find(\"rt-link\", attrs={\"slot\": \"criticName\"})\n            )\n            review[\"Critic\"] = critic_el.get_text(strip=True) if critic_el else \"\"\n\n            # Publication\n            pub_el = (\n                row.find(\"a\", class_=lambda c: c and \"publication\" in c.lower())\n                or row.find(\"em\")\n                or row.find(\"rt-link\", attrs={\"slot\": \"publicationName\"})\n            )\n            review[\"Publication\"] = pub_el.get_text(strip=True) if pub_el else \"\"\n\n            # Date\n            date_el = (\n                row.find(\"span\", class_=lambda c: c and \"date\" in c.lower())\n                or row.find(attrs={\"data-qa\": \"review-date\"})\n                or row.find(\"rt-text\", attrs={\"slot\": \"displayDate\"})\n            )\n            review[\"Date\"] = date_el.get_text(strip=True) if date_el else \"\"\n\n            # Review text\n            text_el = (\n                row.find(\"p\", class_=lambda c: c and \"review\" in c.lower())\n                or row.find(\"div\", class_=lambda c: c and \"review\" in c.lower())\n                or row.find(attrs={\"data-qa\": \"review-text\"})\n                or row.find(\"rt-text\", attrs={\"slot\": \"reviewQuote\"})\n            )\n            review[\"Review Text\"] = text_el.get_text(strip=True) if text_el else \"\"\n\n            # Sentiment (Fresh / Rotten)\n            sentiment = \"\"\n            sent_el = row.find(attrs={\"data-qa\": lambda v: v and \"tomatometer\" in v.lower()}) if row else None\n            if sent_el:\n                sentiment = \"Fresh\" if \"fresh\" in str(sent_el).lower() else \"Rotten\"\n            else:\n                for attr_val in [str(row)]:\n                    if \"fresh\" in attr_val.lower() and \"rotten\" not in attr_val.lower():\n                        sentiment = \"Fresh\"\n                    elif \"rotten\" in attr_val.lower():\n                        sentiment = \"Rotten\"\n            review[\"Sentiment\"] = sentiment\n\n            reviews.append(review)\n        return reviews\n\n    # ── Strategy 2: look for elements with data-qa attributes ──\n    rows = soup.find_all(attrs={\"data-qa\": \"critic-review\"})\n    if not rows:\n        rows = soup.find_all(attrs={\"data-qa\": lambda v: v and \"review\" in v})\n    if rows:\n        print(f\"Strategy 2: Found {len(rows)} data-qa review elements\")\n        for i, row in enumerate(rows, start=1):\n            texts = [t.strip() for t in row.stripped_strings]\n            reviews.append({\n                \"Review ID\": i,\n                \"Critic\": texts[0] if len(texts) > 0 else \"\",\n                \"Publication\": texts[1] if len(texts) > 1 else \"\",\n                \"Date\": texts[-1] if len(texts) > 2 else \"\",\n                \"Review Text\": \" \".join(texts[2:-1]) if len(texts) > 3 else \"\",\n                \"Sentiment\": \"\",\n            })\n        return reviews\n\n    # ── Strategy 3: broad fallback — grab all visible text blocks ──\n    print(\"WARNING: Could not find reviews with known selectors.\")\n    print(\"Run Step 1 with HEADLESS=False and inspect the page manually.\")\n    print(\"Then update the selectors in this function.\")\n    return reviews\n\nreviews = parse_critic_reviews(driver)\nprint(f\"\\nTotal reviews parsed: {len(reviews)}\")\nif reviews:\n    print(f\"\\nSample review:\\n{reviews[0]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Step 4: Save to CSV ───────────────────────────────────────\n\ndf = pd.DataFrame(reviews)\ncsv_filename = f\"{MOVIE_SLUG}_critic_reviews.csv\"\ncsv_path = os.path.join(OUTPUT_DIR, csv_filename)\ndf.to_csv(csv_path, index=False, encoding=\"utf-8\")\nprint(f\"Saved {len(df)} reviews to: {csv_path}\")\ndf.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# ── Step 5: Clean up ──────────────────────────────────────────\n\ndriver.quit()\nprint(\"Browser closed.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}